import { useCallback, useRef } from 'react';
import { useAppDispatch, useAppSelector } from './redux';
import { webrtcService } from '../services/webrtcService';
import { RealtimeEvent, TranslationPair } from '../types';
import { errorHandler, handleWebRTCError, handleTranslationError, handleMedicalActionError } from '../services/errorHandlingService';

// Redux actions
import {
  updateVoiceStatus,
  setConnection,
  setConnected,
  setLastTranslation,
  setPendingUserMessage,
  addToMessageQueue,
  clearMessageQueue,
  setConversationEnded,
  resetVoiceState,
  cleanupConnection,
  sendFunctionCall,
} from '../store/slices/voiceSlice';

import {
  setCurrentSession,
  addMessage,
  addTranslationPair,
  updateTranslationPair,
  clearCurrentSession,
  saveMessage,
  endSession,
  generateSummary,
} from '../store/slices/conversationSlice';

import {
  detectIntent,
  showIntent,
  hideIntent,
} from '../store/slices/intentSlice';

export const useVoiceConversation = () => {
  const dispatch = useAppDispatch();
  
  // Get current state
  const voiceState = useAppSelector((state) => (state.voice as any)?.voiceState);
  const connection = useAppSelector((state) => (state.voice as any)?.connection);
  const currentSession = useAppSelector((state) => (state.conversation as any)?.currentSession);
  const translationPairs = useAppSelector((state) => (state.conversation as any)?.translationPairs || []);
  const lastTranslation = useAppSelector((state) => (state.voice as any)?.lastTranslation || '');
  const pendingUserMessage = useAppSelector((state) => (state.voice as any)?.pendingUserMessage || false);
  const messageQueue = useAppSelector((state) => (state.voice as any)?.messageQueue || []);

  // Refs for managing current translation pair
  const currentPairRef = useRef<string | null>(null);
  const assistantTranscriptRef = useRef<string>('');
  const translationTimeoutRef = useRef<NodeJS.Timeout | null>(null);

  // Check browser compatibility on hook initialization
  const checkBrowserSupport = useCallback(() => {
    const compatibility = errorHandler.checkBrowserCompatibility();
    if (!compatibility.compatible) {
      const errorDetails = errorHandler.handleError(
        new Error(`Browser not supported: ${compatibility.issues.join(', ')}`),
        'browser'
      );
      dispatch(updateVoiceStatus({ 
        status: errorDetails.userMessage, 
        isError: true 
      }));
      return false;
    }
    return true;
  }, [dispatch]);

  // Simplified emergency timeout - only trigger in truly stuck states
  const startEmergencyTimeout = useCallback(() => {
    // Only set timeout if we don't already have one
    if (translationTimeoutRef.current) {
      clearTimeout(translationTimeoutRef.current);
    }
    
    translationTimeoutRef.current = setTimeout(() => {
      console.log('üö® EMERGENCY TIMEOUT: Translation completely stuck, forcing recovery');
      
      // Only trigger if we actually have a stuck translation pair
      if (currentPairRef.current) {
        const fallbackText = assistantTranscriptRef.current || 'Translation timeout';
        console.log('üîß Force completing with fallback:', fallbackText);
        dispatch(updateTranslationPair({
          id: currentPairRef.current,
          translatedText: fallbackText
        }));
      }
      
      // Reset state
      currentPairRef.current = null;
      assistantTranscriptRef.current = '';
      dispatch(setPendingUserMessage(false));
      dispatch(clearMessageQueue());
      
      dispatch(updateVoiceStatus({ 
        status: 'üé§ Ready for medical interpretation...', 
        isListening: false 
      }));
      
    }, 4000); // Reduced to 4 seconds for faster recovery
  }, [dispatch]);

  const clearEmergencyTimeout = useCallback(() => {
    if (translationTimeoutRef.current) {
      clearTimeout(translationTimeoutRef.current);
      translationTimeoutRef.current = null;
    }
  }, []);

  // Enhanced status update 
  const updateVoiceStatusWithTracking = useCallback((payload: { 
    status: string; 
    isError?: boolean; 
    isListening?: boolean 
  }) => {
    dispatch(updateVoiceStatus(payload));
  }, [dispatch]);

  // Create new translation pair
  const createTranslationPair = useCallback((originalText: string, translatedText: string, originalLang: 'English' | 'Spanish', translatedLang: 'English' | 'Spanish') => {
    try {
      const pairId = Date.now().toString();
      currentPairRef.current = pairId;
      
      console.log('üÜî Creating translation pair with ID:', pairId);
      
      dispatch(addTranslationPair({
        id: pairId, // Pass the same ID that we store in currentPairRef
        originalText,
        translatedText,
        originalLang,
        translatedLang,
        timestamp: new Date().toISOString(),
        isComplete: false,
      }));
    } catch (error) {
      console.error('Failed to create translation pair:', error);
    }
  }, [dispatch]);

  // Update assistant transcript (streaming) with error handling
  const updateAssistantTranscript = useCallback((delta: string) => {
    try {
      assistantTranscriptRef.current += delta;
      
      if (currentPairRef.current) {
        console.log('üîÑ Updating translation pair ID:', currentPairRef.current, 'with text:', assistantTranscriptRef.current.slice(0, 50) + '...');
        dispatch(updateTranslationPair({
          id: currentPairRef.current,
          translatedText: assistantTranscriptRef.current,
          isComplete: false
        }));
      } else {
        console.warn('‚ö†Ô∏è No currentPairRef.current available for update');
      }
    } catch (error) {
      handleTranslationError(error);
    }
  }, [dispatch]);

  // Finalize assistant transcript with error handling
  const finalizeAssistantTranscript = useCallback((finalText: string) => {
    try {
      if (currentPairRef.current) {
        console.log('‚úÖ Finalizing translation pair ID:', currentPairRef.current, 'with final text:', finalText);
        dispatch(updateTranslationPair({
          id: currentPairRef.current,
          translatedText: finalText,
          isComplete: true
        }));
      } else {
        console.warn('‚ö†Ô∏è No currentPairRef.current available for finalization');
      }

      // Save assistant message to database (as per API doc) - Enhanced session ID lookup
      const sessionId = connection?.sessionId || currentSession?.session_id || webrtcService.getConnection().sessionId;
      
      if (sessionId && finalText) {
        console.log('üíæ Saving assistant message to database:', {
          sessionId: sessionId,
          content: finalText,
          messageType: 'assistant',
          source: connection?.sessionId ? 'redux-connection' : currentSession?.session_id ? 'redux-session' : 'webrtc-service'
        });
        
        dispatch(saveMessage({
          sessionId: sessionId,
          messageType: 'assistant',
          content: finalText,
          // Add audio duration estimate (approximate)
          audioDuration: finalText.length * 0.1, // Rough estimate: 10 chars per second
          confidenceScore: 0.98 // High confidence for OpenAI responses
        })).then((result) => {
          console.log('‚úÖ Assistant message saved successfully:', result);
        }).catch((error) => {
          console.error('‚ùå Failed to save assistant message:', error);
          console.error('   Session ID used:', sessionId);
          console.error('   API URL:', process.env.REACT_APP_API_URL);
          console.error('   Full error:', error);
          // Continue operation even if save fails
        });
      } else {
        console.warn('‚ö†Ô∏è No session ID or final text for saving assistant message');
        console.warn('   Session ID attempts:', {
          reduxConnection: connection?.sessionId,
          reduxSession: currentSession?.session_id,
          webrtcService: webrtcService.getConnection().sessionId
        });
        console.warn('   Final text:', finalText);
      }

      assistantTranscriptRef.current = '';
      currentPairRef.current = null;
    } catch (error) {
      handleTranslationError(error);
    }
  }, [dispatch, connection]);

  // Process queued messages with error handling
  const processMessageQueue = useCallback(() => {
    try {
      if (messageQueue.length === 0) return;

      console.log(`üìã Processing ${messageQueue.length} queued messages`);
      
      for (const message of messageQueue) {
        if (message.type === 'delta') {
          updateAssistantTranscript(message.content);
        } else if (message.type === 'final') {
          finalizeAssistantTranscript(message.content);
        }
      }
      
      dispatch(clearMessageQueue());
    } catch (error) {
      handleTranslationError(error);
    }
  }, [messageQueue, updateAssistantTranscript, finalizeAssistantTranscript, dispatch]);

  // Detect language (simple heuristic)
  const detectLanguage = useCallback((text: string): boolean => {
    // Simple Spanish detection based on common patterns
    const spanishIndicators = /[√±√°√©√≠√≥√∫√º]|(\b(el|la|es|est√°|son|por|para|con|sin|muy|m√°s|como|qu√©|c√≥mo|d√≥nde|cu√°ndo)\b)/i;
    return spanishIndicators.test(text);
  }, []);

  // Generate summary without ending conversation
  const generateSummaryOnly = useCallback(async () => {
    if (!connection?.sessionId) return;
    
    try {
      console.log('üìã Generating conversation summary...');
      dispatch(updateVoiceStatus({ status: 'üìã Generating medical summary...' }));
      
      const summaryResult = await dispatch(generateSummary(connection.sessionId)).unwrap();
      
      // Set the current session as selectedConversation so the summary shows in UI
      dispatch({
        type: 'conversation/fetchConversationDetails/fulfilled',
        payload: {
          session_id: connection.sessionId,
          summary: summaryResult.summary
        }
      });
      
      dispatch(updateVoiceStatus({ 
        status: '‚úÖ Medical summary saved! You can continue the conversation.', 
        isError: false 
      }));
      
      return summaryResult.summary;
      
    } catch (error) {
      const errorDetails = errorHandler.handleError(error, 'summary');
      dispatch(updateVoiceStatus({ 
        status: errorDetails.userMessage, 
        isError: true 
      }));
    }
  }, [connection, dispatch]);

  // Auto-summarize and end conversation with error handling
  const summarizeAndEndConversation = useCallback(async () => {
    await generateSummaryOnly();
    // Add delay before stopping if needed
    setTimeout(() => {
      stopVoiceChat();
    }, 1000);
  }, [generateSummaryOnly]);

  // Handle function calls (medical actions) with comprehensive error handling
  const handleFunctionCall = useCallback(async (event: any) => {
    const functionName = event.name;
    const argumentsStr = event.arguments || '{}';
    const callId = event.call_id;
    
    console.log('üéØ Function call received:', { functionName, callId });
    console.log('üîç Available session IDs:');
    console.log('   - connection.sessionId:', connection?.sessionId);
    console.log('   - currentSession.session_id:', currentSession?.session_id);
    
    try {
      let argumentsObj = {};
      try {
        argumentsObj = JSON.parse(argumentsStr);
      } catch (e) {
        console.log('Failed to parse arguments, using empty object');
      }
      
      if (functionName === 'send_lab_order') {
        console.log('üß™ SENDING LAB ORDER!');
        dispatch(updateVoiceStatus({ status: 'üß™ Sending lab order...' }));
      } else if (functionName === 'schedule_followup_appointment') {
        console.log('üìÖ SCHEDULING APPOINTMENT!');
        dispatch(updateVoiceStatus({ status: 'üìÖ Scheduling follow-up appointment...' }));
      }
      
      // Send function call to backend - use current session ID from state
      console.log('üîç DEBUG: Checking available session IDs...');
      console.log('   - connection object:', connection);
      console.log('   - connection?.sessionId:', connection?.sessionId);
      console.log('   - currentSession object:', currentSession);
      console.log('   - currentSession?.session_id:', currentSession?.session_id);
      
      // Try multiple sources for session ID
      let sessionId = connection?.sessionId || currentSession?.session_id;
      
      // Fallback: get session ID directly from WebRTC service
      if (!sessionId) {
        const webrtcConnection = webrtcService.getConnection();
        sessionId = webrtcConnection.sessionId;
        console.log('üîÑ Fallback: Got session ID from WebRTC service:', sessionId);
      }
      
      console.log('   - Final sessionId:', sessionId);
      
      if (!sessionId) {
        console.error('‚ùå No session ID available from any source!');
        console.error('   Redux connection state:', connection);
        console.error('   Redux session state:', currentSession);
        console.error('   WebRTC service connection:', webrtcService.getConnection());
        throw new Error('No active session found. Please start a new medical interpretation session.');
      }
      
      console.log('üì§ Sending function call with session ID:', sessionId);
      
      const result = await dispatch(sendFunctionCall({
        functionName,
        args: argumentsObj,
        callId,
        sessionId
      })).unwrap();
      
      // Save medical action to conversation history (as per API doc)
      const actionMessage = `üîß Medical Action: ${functionName} executed successfully`;
      dispatch(saveMessage({
        sessionId,
        messageType: 'system',
        content: actionMessage
      })).catch((error) => {
        console.warn('Failed to save medical action message:', error);
      });
      
      // Send function result back to OpenAI
      const functionOutput = {
        type: "conversation.item.create",
        item: {
          type: "function_call_output",
          call_id: callId,
          output: JSON.stringify(result)
        }
      };
      
      webrtcService.sendMessage(functionOutput);
      
      // Request "Done" response
      const responseRequest = {
        type: "response.create",
        response: {
          modalities: ["text", "audio"],
          instructions: "ONLY say 'Done' - one word only. Do NOT say anything else. Do NOT provide explanations. Do NOT translate. Just 'Done'."
        }
      };
      
      webrtcService.sendMessage(responseRequest);
      
      // Auto-generate summary after medical action (without ending conversation) - as per API doc
      setTimeout(async () => {
        try {
          console.log('üìã Auto-generating medical summary after action...');
          const summaryResult = await dispatch(generateSummary(sessionId)).unwrap();
          console.log('‚úÖ Medical summary generated successfully:', summaryResult);
          
          // Store summary in UI state (following API doc structure)
          dispatch({
            type: 'conversation/fetchConversationDetails/fulfilled',
            payload: {
              session_id: sessionId,
              summary: summaryResult.summary
            }
          });
          
          // Save summary generation event to conversation history
          const summaryMessage = `üìã Medical summary automatically generated after ${functionName}`;
          dispatch(saveMessage({
            sessionId,
            messageType: 'system',
            content: summaryMessage
          })).catch((error) => {
            console.warn('Failed to save summary event message:', error);
          });
          
        } catch (summaryError) {
          console.warn('Failed to generate auto-summary:', summaryError);
          // Don't fail the whole operation if summary generation fails
        }
      }, 2000);
      
    } catch (error) {
      const errorDetails = handleMedicalActionError(
        error, 
        functionName === 'send_lab_order' ? 'lab-order' : 'appointment'
      );
      
      dispatch(updateVoiceStatus({ 
        status: errorDetails.userMessage, 
        isError: true 
      }));

      // Provide recovery suggestions
      const recoveryActions = errorHandler.getRecoveryActions(errorDetails.code);
      console.log('üîß Recovery suggestions:', recoveryActions);
    }
  }, [dispatch, connection, currentSession, generateSummaryOnly]);

  // Handle user transcript (speech recognition result) with error handling
  const handleUserTranscript = useCallback((transcript: string) => {
    try {
      // Detect and update intent
      dispatch(detectIntent(transcript));
      dispatch(showIntent());

      // Check for repeat requests
      const text = transcript.toLowerCase().trim();
      const repeatKeywords = ['repeat that', 'repeat', 'say again', 'repite eso', 'repite', 'otra vez', 'rep√≠telo', 'dilo otra vez'];
      const isRepeatRequest = repeatKeywords.some(keyword => text.includes(keyword));

      if (isRepeatRequest && lastTranslation) {
        console.log('üîÑ REPEAT REQUEST DETECTED! Repeating:', lastTranslation);
        dispatch(updateVoiceStatus({ status: 'üîÑ Repeating last translation...' }));
        
        // Create new translation pair for repeat
        createTranslationPair(transcript, '', 'English', 'Spanish');
        
        // Send repeat instruction to OpenAI
        const repeatMessage = {
          type: "conversation.item.create",
          item: {
            type: "message",
            role: "user",
            content: [{
              type: "input_text", 
              text: `REPEAT_COMMAND: Please repeat exactly: "${lastTranslation}"`
            }]
          }
        };
        
        webrtcService.sendMessage(repeatMessage);
        
        const responseRequest = {
          type: "response.create",
          response: {
            modalities: ["text", "audio"],
            instructions: `Say exactly this again: "${lastTranslation}". Do not translate this instruction - just repeat the translation.`
          }
        };
        
        webrtcService.sendMessage(responseRequest);
        dispatch(setPendingUserMessage(false));
        return;
      }

      // Create new translation pair for regular speech
      const isSpanish = detectLanguage(transcript);
      const originalLang = isSpanish ? 'Spanish' : 'English';
      const translatedLang = isSpanish ? 'English' : 'Spanish';
      
      createTranslationPair(transcript, '', originalLang, translatedLang);

      // Save user message to database (as per API doc) - Enhanced session ID lookup
      const sessionId = connection?.sessionId || currentSession?.session_id || webrtcService.getConnection().sessionId;
      
      if (sessionId) {
        console.log('üíæ Saving user message to database:', {
          sessionId: sessionId,
          content: transcript,
          messageType: 'user',
          source: connection?.sessionId ? 'redux-connection' : currentSession?.session_id ? 'redux-session' : 'webrtc-service'
        });
        
        dispatch(saveMessage({
          sessionId: sessionId,
          messageType: 'user',
          content: transcript,
          // Add confidence score if available from speech recognition
          confidenceScore: 0.95 // Default high confidence for successful transcription
        })).then((result) => {
          console.log('‚úÖ User message saved successfully:', result);
        }).catch((error) => {
          console.error('‚ùå Failed to save user message:', error);
          console.error('   Session ID used:', sessionId);
          console.error('   API URL:', process.env.REACT_APP_API_URL);
          console.error('   Full error:', error);
          // Continue operation even if save fails
        });
      } else {
        console.warn('‚ö†Ô∏è No session ID available from ANY source for saving user message');
        console.warn('   Redux connection:', connection);
        console.warn('   Redux session:', currentSession);
        console.warn('   WebRTC service:', webrtcService.getConnection());
      }

      dispatch(setPendingUserMessage(false));
      setTimeout(() => processMessageQueue(), 100);
    } catch (error) {
      handleTranslationError(error);
    }
  }, [dispatch, lastTranslation, connection, createTranslationPair, detectLanguage, processMessageQueue]);

  // Handle real-time events from OpenAI with comprehensive error handling
  const handleRealtimeEvent = useCallback((event: RealtimeEvent) => {
    try {
      console.log('üì® Processing event:', event.type, event);

      switch (event.type) {
        case 'session.created':
          console.log('‚úÖ OpenAI Session created:', event.session?.id);
          dispatch(updateVoiceStatus({ 
            status: '‚úÖ Connected to OpenAI! Start speaking...', 
            isListening: false 
          }));
          break;

        case 'session.updated':
          console.log('‚öôÔ∏è OpenAI Session updated:', event);
          dispatch(updateVoiceStatus({ 
            status: 'üé§ Medical interpreter ready! Start speaking...', 
            isListening: false 
          }));
          break;

        case 'input_audio_buffer.speech_started':
          console.log('üé§ Speech started detected');
          dispatch(updateVoiceStatus({ 
            status: 'üé§ Listening for interpretation...', 
            isListening: true 
          }));
          dispatch(setPendingUserMessage(true));
          break;

        case 'input_audio_buffer.speech_stopped':
          console.log('üîá Speech stopped detected');
          updateVoiceStatusWithTracking({ 
            status: 'üîÑ Processing and translating...', 
            isListening: false 
          });
          break;

        case 'input_audio_buffer.committed':
          console.log('üíæ Audio buffer committed to OpenAI');
          break;

        case 'conversation.item.created':
          console.log('üìù Conversation item created:', event);
          break;

        case 'response.created':
          console.log('üîÑ OpenAI response created:', event);
          updateVoiceStatusWithTracking({ 
            status: 'üó£Ô∏è Generating translation...', 
            isListening: false 
          });
          
          // Start emergency timeout only if needed
          startEmergencyTimeout();
          
          // Add a shorter timeout to check if we're getting deltas
          setTimeout(() => {
            if (currentPairRef.current && assistantTranscriptRef.current === '') {
              console.log('‚ö†Ô∏è No deltas received after 2 seconds, translation may be stuck');
              console.log('üîß Current pair:', currentPairRef.current);
              console.log('üîß Current transcript:', assistantTranscriptRef.current);
            }
          }, 2000);
          break;

        case 'response.output_item.added':
          console.log('üì§ Response output item added:', event);
          break;

        case 'response.content_part.added':
          console.log('üìù Response content part added:', event);
          break;

        case 'response.audio_transcript.delta':
          console.log('üî§ Audio transcript delta:', event.delta);
          console.log('üîç Current pair ref:', currentPairRef.current);
          console.log('üîç Assistant transcript so far:', assistantTranscriptRef.current);
          if (event.delta && event.delta.trim()) {
            console.log('üìù Updating assistant transcript with delta');
            updateAssistantTranscript(event.delta);
          } else {
            console.log('‚ö†Ô∏è Empty or invalid delta received');
          }
          break;

        case 'response.audio_transcript.done':
          console.log('‚úÖ Audio transcript completed:', event.transcript);
          if (event.transcript && event.transcript.trim()) {
            dispatch(setLastTranslation(event.transcript.trim()));
            console.log('‚úÖ Finalizing assistant transcript');
            finalizeAssistantTranscript(event.transcript);
          }
          
          // Clear emergency timeout and update status
          clearEmergencyTimeout();
          updateVoiceStatusWithTracking({ 
            status: '‚úÖ Translation complete! Ready for next speaker...', 
            isListening: false 
          });
          break;

        case 'response.done':
          console.log('üèÅ OpenAI response completed');
          console.log('üîç Final assistant transcript:', assistantTranscriptRef.current);
          console.log('üîç Current pair ref:', currentPairRef.current);
          
          // If we have a pair but no transcript, something went wrong
          if (currentPairRef.current && assistantTranscriptRef.current === '') {
            console.log('üö® Response done but no transcript received! Forcing fallback completion');
            dispatch(updateTranslationPair({
              id: currentPairRef.current,
              translatedText: 'Translation not received - please try again',
              isComplete: true
            }));
          }
          
          // Clear any emergency timeouts and ensure ready state
          clearEmergencyTimeout();
          dispatch(setPendingUserMessage(false));
          assistantTranscriptRef.current = '';
          currentPairRef.current = null;
          
          // Set ready state
          dispatch(updateVoiceStatus({ 
            status: 'üé§ Ready for medical interpretation...', 
            isListening: false 
          }));
          break;

        case 'conversation.item.input_audio_transcription.completed':
          console.log('üìù User speech transcription completed:', event.transcript);
          if (event.transcript) {
            console.log('üó£Ô∏è User said:', event.transcript);
            console.log('üîç About to call handleUserTranscript...');
            
            // Check if this should trigger a function call
            const text = event.transcript.toLowerCase();
            if (text.includes('send lab order') || text.includes('order tests') || text.includes('get labs')) {
              console.log('üö® FUNCTION TRIGGER DETECTED in transcript:', event.transcript);
              console.log('üö® OpenAI should call send_lab_order function now...');
            }
            if (text.includes('schedule follow') || text.includes('next appointment') || text.includes('come back')) {
              console.log('üö® APPOINTMENT TRIGGER DETECTED in transcript:', event.transcript);
              console.log('üö® OpenAI should call schedule_followup_appointment function now...');
            }
            
            handleUserTranscript(event.transcript);
            console.log('‚úÖ handleUserTranscript completed');
          }
          break;

        case 'conversation.item.input_audio_transcription.failed':
          console.error('‚ùå Speech transcription failed:', event);
          dispatch(updateVoiceStatus({ 
            status: '‚ùå Speech recognition failed. Please try again.', 
            isError: true 
          }));
          break;

        case 'response.function_call_arguments.delta':
          console.log('üéØ Function call arguments delta:', event);
          break;

        case 'response.function_call_arguments.done':
          console.log('üéØ FUNCTION CALL DETECTED!', event);
          console.log('üéØ Function name:', event.name);
          console.log('üéØ Function arguments:', event.arguments);
          handleFunctionCall(event);
          break;

        case 'rate_limits.updated':
          console.log('üìä Rate limits updated:', event);
          break;

        case 'error':
          console.error('‚ùå OpenAI error:', event.error);
          const errorDetails = handleTranslationError(new Error(event.error?.message || 'OpenAI API error'));
          dispatch(updateVoiceStatus({ 
            status: errorDetails.userMessage, 
            isError: true 
          }));
          break;

        default:
          console.log('üì® Unhandled OpenAI event:', event.type, event);
          // Check if this is a response-related event we might be missing
          if (event.type.includes('response') || event.type.includes('audio')) {
            console.log('üö® POTENTIALLY MISSED RESPONSE EVENT:', event.type, event);
          }
      }
    } catch (error) {
      console.error('üí• Error handling realtime event:', error);
      handleTranslationError(error);
    }
  }, [dispatch, pendingUserMessage, handleUserTranscript, updateAssistantTranscript, finalizeAssistantTranscript, handleFunctionCall, processMessageQueue]);

  // Stop voice chat with comprehensive cleanup
  const stopVoiceChat = useCallback(async () => {
    try {
      console.log('üõë Stopping voice chat...');
      dispatch(updateVoiceStatus({ status: 'üíæ Saving conversation and generating summary...' }));
      
      // Get session ID from multiple sources
      const sessionId = connection?.sessionId || currentSession?.session_id || webrtcService.getConnection().sessionId;
      
      if (sessionId) {
        try {
          console.log('üìã Ending session and generating final summary...');
          
          // 1. End session in database (as per API doc)
          await dispatch(endSession(sessionId)).unwrap();
          console.log('‚úÖ Session ended successfully');
          
          // 2. Generate final medical summary (as per API doc)
          const summaryResult = await dispatch(generateSummary(sessionId)).unwrap();
          console.log('‚úÖ Final summary generated successfully');
          
          // 3. Update UI with final summary
          dispatch({
            type: 'conversation/fetchConversationDetails/fulfilled',
            payload: {
              session_id: sessionId,
              summary: summaryResult.summary
            }
          });
          
        } catch (error) {
          console.error('Failed to properly end session:', error);
          // Continue cleanup even if database operations fail
        }
      } else {
        console.warn('‚ö†Ô∏è No session ID found for cleanup');
      }
      
      // Cleanup WebRTC
      webrtcService.cleanup();
      
      // Reset Redux state  
      dispatch(cleanupConnection());
      dispatch(resetVoiceState());
      dispatch(clearCurrentSession());
      dispatch(hideIntent());
      
      // Reset refs and clear timeouts
      currentPairRef.current = null;
      assistantTranscriptRef.current = '';
      if (translationTimeoutRef.current) {
        clearTimeout(translationTimeoutRef.current);
        translationTimeoutRef.current = null;
      }
      
      // Clear any emergency timeout
      clearEmergencyTimeout();
      
      dispatch(updateVoiceStatus({ status: 'Session ended! Summary saved to history. üìã' }));
      
      console.log('‚úÖ Voice chat stopped and session properly saved');
      
    } catch (error) {
      const errorDetails = errorHandler.handleError(error, 'cleanup');
      dispatch(updateVoiceStatus({ 
        status: errorDetails.userMessage, 
        isError: true 
      }));
    }
  }, [dispatch, connection, currentSession]);

  // Start voice chat with comprehensive error handling
  const startVoiceChat = useCallback(async () => {
    try {
      // Check browser compatibility first
      if (!checkBrowserSupport()) {
        return;
      }

      dispatch(resetVoiceState());
      dispatch(updateVoiceStatus({ status: 'üéôÔ∏è Getting microphone access...' }));
      
      // Set up event handlers
      webrtcService.onEvent(handleRealtimeEvent);
      webrtcService.onConnectionStateChange((state) => {
        dispatch(updateVoiceStatus({ status: `üîó Connection: ${state}` }));
        if (state === 'connected') {
          dispatch(setConnected(true));
          dispatch(updateVoiceStatus({ 
            status: '‚úÖ Connected to Medical Interpreter! Start speaking!', 
            isError: false 
          }));
          dispatch(showIntent());
        } else if (state === 'failed') {
          const errorDetails = handleWebRTCError(new Error('WebRTC connection failed'));
          dispatch(updateVoiceStatus({ 
            status: errorDetails.userMessage, 
            isError: true 
          }));
          dispatch(setConnected(false));
        }
      });
      
      // Start WebRTC connection
      const connectionResult = await webrtcService.startConnection();
      
      console.log('üîó WebRTC connection result:', connectionResult);
      console.log('üìã Session ID from connection:', connectionResult.sessionId);
      console.log('üîç Connection result type:', typeof connectionResult);
      console.log('üîç Connection result keys:', Object.keys(connectionResult));
      
      // Store only serializable connection data in Redux
      const serializableConnection = {
        sessionId: connectionResult.sessionId,
        isConnected: true,
        connectionState: 'connected'
      };
      
      console.log('üíæ Storing connection in Redux:', serializableConnection);
      dispatch(setConnection(serializableConnection));
      
      // Emergency timeout will be started per response
      console.log('üõ°Ô∏è Anti-stuck system ready');
      
      // Create session if we have a session ID
      if (connectionResult.sessionId) {
        const sessionData = {
          session_id: connectionResult.sessionId,
          started_at: new Date().toISOString(),
          total_messages: 0,
          is_active: true,
        };
        
        console.log('‚úÖ Setting current session with data:', sessionData);
        dispatch(setCurrentSession(sessionData));
        
        // Verify session was set
        setTimeout(() => {
          console.log('üîç Verifying session was set in Redux...');
          console.log('   Current connection from Redux:', connection);
          console.log('   Current session from Redux:', currentSession);
        }, 100);
        
      } else {
        console.warn('‚ö†Ô∏è No session ID received from WebRTC service!');
        console.warn('   WebRTC result was:', connectionResult);
      }
      
    } catch (error) {
      const errorDetails = handleWebRTCError(error);
      dispatch(updateVoiceStatus({ 
        status: errorDetails.userMessage, 
        isError: true 
      }));

      // Provide recovery suggestions for common issues
      if ((error as Error).name === 'NotAllowedError') {
        const recoveryActions = errorHandler.getRecoveryActions('MICROPHONE_ACCESS_DENIED');
        console.log('üîß Microphone access recovery steps:', recoveryActions);
      }
    }
  }, [dispatch, handleRealtimeEvent, checkBrowserSupport]);

  return {
    startVoiceChat,
    stopVoiceChat,
    voiceState,
    connection,
    translationPairs,
    currentSession,
  };
}; 